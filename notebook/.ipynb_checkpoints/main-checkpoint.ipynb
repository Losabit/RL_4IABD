{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi(s,a)\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy = Dict[int, Dict[int, float]]\n",
    "\n",
    "# V(s)\n",
    "ValueFunction = Dict[int, float]\n",
    "\n",
    "# Q(s,a)\n",
    "ActionValueFunction = Dict[int, Dict[int, float]]\n",
    "\n",
    "\n",
    "# Pi(s,a) and V(s)\n",
    "@dataclass\n",
    "class PolicyAndValueFunction:\n",
    "    pi: Policy\n",
    "    v: ValueFunction\n",
    "\n",
    "\n",
    "# Pi(s,a) and Q(s,a)\n",
    "@dataclass\n",
    "class PolicyAndActionValueFunction:\n",
    "    pi: Policy\n",
    "    q: ActionValueFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDPEnv:\n",
    "    def states(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        pass\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        pass\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SingleAgentEnv:\n",
    "    def state_id(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        pass\n",
    "\n",
    "    def score(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    def reset_random(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DeepSingleAgentWithDiscreteActionsEnv:\n",
    "    def state_description(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def state_description_length(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def max_actions_count(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        pass\n",
    "\n",
    "    def score(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineWorld(MDPEnv):\n",
    "    def __init__(self, cells_count: int):\n",
    "        self.cells_count = cells_count\n",
    "        self.__states = np.arange(self.cells_count)\n",
    "        self.__actions = np.array([0, 1])\n",
    "        self.__rewards = np.array([-1, 0, 1])\n",
    "        self.probality = self.probality_setup()\n",
    "\n",
    "    def probality_setup(self):\n",
    "        p = np.zeros((len(self.__states), len(self.__actions), len(self.__states), len(self.__rewards)))\n",
    "        for s in range(1, self.cells_count - 2):\n",
    "            p[s, 1, s + 1, 1] = 1.0\n",
    "\n",
    "        for s in range(2, self.cells_count - 1):\n",
    "            p[s, 0, s - 1, 1] = 1.0\n",
    "\n",
    "        p[self.cells_count - 2, 1, self.cells_count - 1, 2] = 1.0\n",
    "        p[1, 0, 0, 0] = 1.0\n",
    "        return p\n",
    "\n",
    "    def states(self) -> np.ndarray:\n",
    "        return self.__states\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        return self.__actions\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        return self.__rewards\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        return s == self.cells_count - 1 or s == 0\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        return self.probality[s, a, s_p, r]\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GridWorld(MDPEnv):\n",
    "    def __init__(self, lines: int, columns: int):\n",
    "        self.lines = lines\n",
    "        self.columns = columns\n",
    "        self.cells_count = lines * columns\n",
    "        self.negative_terminal = 4\n",
    "        self.positive_terminal = self.cells_count - 1\n",
    "        self.__states = np.arange(self.cells_count)\n",
    "        self.__actions = np.array([0, 1, 2, 3])\n",
    "        self.__rewards = np.array([-1, 0, 1])\n",
    "        self.probality = self.probality_setup()\n",
    "\n",
    "    def probality_setup(self):\n",
    "        p = np.zeros((len(self.__states), len(self.__actions), len(self.__states), len(self.__rewards)))\n",
    "        for line in range(0, self.lines):\n",
    "            for column in range(0, self.columns - 1):\n",
    "                s = line * self.columns + column\n",
    "                if s != self.negative_terminal and s != self.positive_terminal:\n",
    "                    if s + 1 == self.positive_terminal:\n",
    "                        p[s, 1, s + 1, 2] = 1.0\n",
    "                    elif s + 1 == self.negative_terminal:\n",
    "                        p[s, 1, s + 1, 0] = 1.0\n",
    "                    else:\n",
    "                        p[s, 1, s + 1, 1] = 1.0\n",
    "\n",
    "            for column in range(1, self.columns):\n",
    "                s = line * self.columns + column\n",
    "                if s != self.positive_terminal and s != self.negative_terminal:\n",
    "                    if s - 1 == self.negative_terminal:\n",
    "                        p[s, 0, s - 1, 0] = 1.0\n",
    "                    elif s - 1 == self.positive_terminal:\n",
    "                        p[s, 0, s - 1, 2] = 1.0\n",
    "                    else:\n",
    "                        p[s, 0, s - 1, 1] = 1.0\n",
    "\n",
    "        for column in range(0, self.columns):\n",
    "            for line in range(0, self.lines - 1):\n",
    "                s = self.columns * line + column\n",
    "                s2 = self.columns * (line + 1) + column\n",
    "                # up\n",
    "                if s2 != self.positive_terminal and s2 != self.negative_terminal:\n",
    "                    if s == self.negative_terminal:\n",
    "                        p[s2, 2, s, 0] = 1.0\n",
    "                    elif s == self.positive_terminal:\n",
    "                        p[s2, 2, s, 2] = 1.0\n",
    "                    else:\n",
    "                        p[s2, 2, s, 1] = 1.0\n",
    "\n",
    "                # down\n",
    "                if s != self.negative_terminal and s != self.positive_terminal:\n",
    "                    if s2 == self.positive_terminal:\n",
    "                        p[s, 3, s2, 2] = 1.0\n",
    "                    elif s2 == self.negative_terminal:\n",
    "                        p[s, 3, s2, 0] = 1.0\n",
    "                    else:\n",
    "                        p[s, 3, s2, 1] = 1.0\n",
    "        return p\n",
    "\n",
    "    def states(self) -> np.ndarray:\n",
    "        return self.__states\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        return self.__actions\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        return self.__rewards\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        return s == self.positive_terminal or s == self.negative_terminal\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        return self.probality[s, a, s_p, r]\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef policy_evaluation_on_secret_env1() -> ValueFunction:\\n    env = Env1()\\n    pi = np.ones((len(env.states()), len(env.actions())))\\n    pi /= len(env.actions())\\n\\n    theta = 0.00001\\n    gamma = 1.0\\n\\n    V = np.zeros((len(env.states()),))\\n    while True:\\n        delta = 0\\n        for s in env.states():\\n            old_v = V[s]\\n            V[s] = 0.0\\n            for a in env.actions():\\n                for s_next in env.states():\\n                    for r_idx, r in enumerate(env.rewards()):\\n                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\\n            delta = max(delta, abs(V[s] - old_v))\\n\\n        if delta < theta:\\n            break\\n\\n    return dict(enumerate(V.flatten(), 1))\\n\\n\\ndef policy_iteration_on_secret_env1() -> PolicyAndValueFunction:\\n    env = Env1()\\n    V = np.zeros((len(env.states()),))\\n    pi = np.ones((len(env.states()), len(env.actions())))\\n    pi /= len(env.actions())\\n\\n    theta = 0.00001\\n    gamma = 1.0\\n\\n    while True:\\n        while True:\\n            delta = 0\\n            for s in env.states():\\n                old_v = V[s]\\n                V[s] = 0.0\\n                for a in env.actions():\\n                    for s_next in env.states():\\n                        for r_idx, r in enumerate(env.rewards()):\\n                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\\n                delta = max(delta, abs(V[s] - old_v))\\n\\n            if delta < theta:\\n                break\\n\\n        policy_stable = True\\n        for s in env.states():\\n            old_policy = pi[s, :]\\n\\n            best_a = None\\n            best_a_value = None\\n            for a in env.actions():\\n                a_value = 0\\n                for s_p in env.states():\\n                    for r_idx, r in enumerate(env.rewards()):\\n                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\\n                if best_a_value is None or best_a_value < a_value:\\n                    best_a_value = a_value\\n                    best_a = a\\n\\n            pi[s, :] = 0.0\\n            pi[s, best_a] = 1.0\\n            if not np.array_equal(pi[s], old_policy):\\n                policy_stable = False\\n\\n        if policy_stable:\\n            break\\n\\n    final_pi = {}\\n    for indice, value in enumerate(pi):\\n        final_pi[indice] = dict(enumerate(value.flatten(), 1))\\n\\n    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\\n\\n\\ndef value_iteration_on_secret_env1() -> PolicyAndValueFunction:\\n    env = Env1()\\n    V = np.zeros((len(env.states()),))\\n    pi = np.ones((len(env.states()), len(env.actions())))\\n    pi /= len(env.actions())\\n    pi2 = pi.copy()\\n\\n    theta = 0.00001\\n    gamma = 1.0\\n\\n    while True:\\n        delta = 0\\n        for s in env.states():\\n            old_v = V[s]\\n            V[s] = 0.0\\n            best_a_value = None\\n            best_a = None\\n            for a in env.actions():\\n                a_value = 0\\n                for s_p in env.states():\\n                    for r_idx, r in enumerate(env.rewards()):\\n                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\\n                        a_value += pre_a_value\\n                        V[s] += pi[s, a] * pre_a_value\\n                if best_a_value is None or best_a_value < a_value:\\n                    best_a_value = a_value\\n                    best_a = a\\n\\n            delta = max(delta, abs(V[s] - old_v))\\n            pi2[s, :] = 0.0\\n            pi2[s, best_a] = 1.0\\n\\n        if delta < theta:\\n            break\\n\\n    final_pi = {}\\n    for indice, value in enumerate(pi2):\\n        final_pi[indice] = dict(enumerate(value.flatten(), 1))\\n\\n    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def policy_evaluation_on_line_world() -> ValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Line World of 7 cells (leftmost and rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Evaluation Algorithm in order to find the Value Function of a uniform random policy\n",
    "    Returns the Value function (V(s)) of this policy\n",
    "    \"\"\"\n",
    "    env = LineWorld(7)\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            for a in env.actions():\n",
    "                for s_next in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return dict(enumerate(V.flatten(), 1))\n",
    "\n",
    "\n",
    "def policy_iteration_on_line_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Line World of 7 cells (leftmost and rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = LineWorld(7)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in env.states():\n",
    "                old_v = V[s]\n",
    "                V[s] = 0.0\n",
    "                for a in env.actions():\n",
    "                    for s_next in env.states():\n",
    "                        for r_idx, r in enumerate(env.rewards()):\n",
    "                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in env.states():\n",
    "            old_policy = pi[s, :]\n",
    "\n",
    "            best_a = None\n",
    "            best_a_value = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            pi[s, :] = 0.0\n",
    "            pi[s, best_a] = 1.0\n",
    "            if not np.array_equal(pi[s], old_policy):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def value_iteration_on_line_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Line World of 7 cells (leftmost and rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Value Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = LineWorld(7)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "    pi2 = pi.copy()\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            best_a_value = None\n",
    "            best_a = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                        a_value += pre_a_value\n",
    "                        V[s] += pi[s, a] * pre_a_value\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "            pi2[s, :] = 0.0\n",
    "            pi2[s, best_a] = 1.0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi2):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def policy_evaluation_on_grid_world() -> ValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Grid World of 5x5 cells (upper rightmost and lower rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Evaluation Algorithm in order to find the Value Function of a uniform random policy\n",
    "    Returns the Value function (V(s)) of this policy\n",
    "    \"\"\"\n",
    "    env = GridWorld(5, 5)\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            for a in env.actions():\n",
    "                for s_next in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return dict(enumerate(V.flatten(), 1))\n",
    "\n",
    "\n",
    "def policy_iteration_on_grid_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Grid World of 5x5 cells (upper rightmost and lower rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = GridWorld(5, 5)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in env.states():\n",
    "                old_v = V[s]\n",
    "                V[s] = 0.0\n",
    "                for a in env.actions():\n",
    "                    for s_next in env.states():\n",
    "                        for r_idx, r in enumerate(env.rewards()):\n",
    "                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in env.states():\n",
    "            old_policy = pi[s, :]\n",
    "\n",
    "            best_a = None\n",
    "            best_a_value = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            pi[s, :] = 0.0\n",
    "            pi[s, best_a] = 1.0\n",
    "            if not np.array_equal(pi[s], old_policy):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def value_iteration_on_grid_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Grid World of 5x5 cells (upper rightmost and lower rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Value Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = GridWorld(5, 5)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "    pi2 = pi.copy()\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            best_a_value = None\n",
    "            best_a = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                        a_value += pre_a_value\n",
    "                        V[s] += pi[s, a] * pre_a_value\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "            pi2[s, :] = 0.0\n",
    "            pi2[s, best_a] = 1.0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi2):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\"\"\"\n",
    "def policy_evaluation_on_secret_env1() -> ValueFunction:\n",
    "    env = Env1()\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            for a in env.actions():\n",
    "                for s_next in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return dict(enumerate(V.flatten(), 1))\n",
    "\n",
    "\n",
    "def policy_iteration_on_secret_env1() -> PolicyAndValueFunction:\n",
    "    env = Env1()\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in env.states():\n",
    "                old_v = V[s]\n",
    "                V[s] = 0.0\n",
    "                for a in env.actions():\n",
    "                    for s_next in env.states():\n",
    "                        for r_idx, r in enumerate(env.rewards()):\n",
    "                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in env.states():\n",
    "            old_policy = pi[s, :]\n",
    "\n",
    "            best_a = None\n",
    "            best_a_value = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            pi[s, :] = 0.0\n",
    "            pi[s, best_a] = 1.0\n",
    "            if not np.array_equal(pi[s], old_policy):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def value_iteration_on_secret_env1() -> PolicyAndValueFunction:\n",
    "    env = Env1()\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "    pi2 = pi.copy()\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            best_a_value = None\n",
    "            best_a = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                        a_value += pre_a_value\n",
    "                        V[s] += pi[s, a] * pre_a_value\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "            pi2[s, :] = 0.0\n",
    "            pi2[s, best_a] = 1.0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi2):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result fort Dynamic programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.0, 2: -0.666686199082779, 3: -0.3333626319575018, 4: -2.9298624168505594e-05, 5: 0.33331135936520695, 6: 0.6666556796826035, 7: 0.0}\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0}, 1: {1: 0.0, 2: 1.0}, 2: {1: 0.0, 2: 1.0}, 3: {1: 0.0, 2: 1.0}, 4: {1: 0.0, 2: 1.0}, 5: {1: 0.0, 2: 1.0}, 6: {1: 1.0, 2: 0.0}}, v={1: 0.0, 2: -0.666686199082779, 3: -0.3333626319575018, 4: -2.9298624168505594e-05, 5: 0.33331135936520695, 6: 0.6666556796826035, 7: 0.0})\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0}, 1: {1: 0.0, 2: 1.0}, 2: {1: 0.0, 2: 1.0}, 3: {1: 0.0, 2: 1.0}, 4: {1: 0.0, 2: 1.0}, 5: {1: 0.0, 2: 1.0}, 6: {1: 1.0, 2: 0.0}}, v={1: 0.0, 2: -0.666686199082779, 3: -0.3333626319575018, 4: -2.9298624168505594e-05, 5: 0.33331135936520695, 6: 0.6666556796826035, 7: 0.0})\n",
      "{1: -0.012408241005487585, 2: -0.03849208733039875, 3: -0.10946152767210533, 4: -0.3206577559932595, 5: 0.0, 6: -0.011130448543747447, 7: -0.032086147618454595, 8: -0.07868611730403527, 9: -0.1731645441465791, 10: -0.2932932689068334, 11: -1.445875184277138e-05, 12: -2.1020462169746984e-05, 13: -2.0020788780394233e-05, 14: -1.3950568206126668e-05, 15: -6.257202351436786e-06, 16: 0.011105162750401581, 17: 0.03204946364618418, 18: 0.07865141280595922, 19: 0.17314092624874877, 20: 0.2932836672615993, 21: 0.012393199830108002, 22: 0.03847039442485897, 23: 0.10944146483060663, 24: 0.3206455977698388, 25: 0.0}\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 1: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 2: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 3: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 4: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 5: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 6: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 7: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 8: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 9: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 10: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 11: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 12: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 13: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 14: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 15: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 16: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 17: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 18: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 19: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 20: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 21: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 22: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 23: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 24: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}}, v={1: -0.012408241005487585, 2: -0.03849208733039875, 3: -0.10946152767210533, 4: -0.3206577559932595, 5: 0.0, 6: -0.011130448543747447, 7: -0.032086147618454595, 8: -0.07868611730403527, 9: -0.1731645441465791, 10: -0.2932932689068334, 11: -1.445875184277138e-05, 12: -2.1020462169746984e-05, 13: -2.0020788780394233e-05, 14: -1.3950568206126668e-05, 15: -6.257202351436786e-06, 16: 0.011105162750401581, 17: 0.03204946364618418, 18: 0.07865141280595922, 19: 0.17314092624874877, 20: 0.2932836672615993, 21: 0.012393199830108002, 22: 0.03847039442485897, 23: 0.10944146483060663, 24: 0.3206455977698388, 25: 0.0})\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 1: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 2: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 3: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 4: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 5: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 6: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 7: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 8: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 9: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 10: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 11: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 12: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 13: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 14: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 15: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 16: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 17: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 18: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 19: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 20: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 21: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 22: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 23: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 24: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}}, v={1: -0.012408241005487585, 2: -0.03849208733039875, 3: -0.10946152767210533, 4: -0.3206577559932595, 5: 0.0, 6: -0.011130448543747447, 7: -0.032086147618454595, 8: -0.07868611730403527, 9: -0.1731645441465791, 10: -0.2932932689068334, 11: -1.445875184277138e-05, 12: -2.1020462169746984e-05, 13: -2.0020788780394233e-05, 14: -1.3950568206126668e-05, 15: -6.257202351436786e-06, 16: 0.011105162750401581, 17: 0.03204946364618418, 18: 0.07865141280595922, 19: 0.17314092624874877, 20: 0.2932836672615993, 21: 0.012393199830108002, 22: 0.03847039442485897, 23: 0.10944146483060663, 24: 0.3206455977698388, 25: 0.0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(policy_evaluation_on_secret_env1())\\nprint(policy_iteration_on_secret_env1())\\nprint(value_iteration_on_secret_env1())\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    print(policy_evaluation_on_line_world())\n",
    "    print(policy_iteration_on_line_world())\n",
    "    print(value_iteration_on_line_world())\n",
    "\n",
    "    print(policy_evaluation_on_grid_world())\n",
    "    print(policy_iteration_on_grid_world())\n",
    "    print(value_iteration_on_grid_world())\n",
    "\"\"\"\n",
    "    print(policy_evaluation_on_secret_env1())\n",
    "    print(policy_iteration_on_secret_env1())\n",
    "    print(value_iteration_on_secret_env1())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tictactoe Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tic_tac_toe_dict():\n",
    "    dict = {}\n",
    "    all_possible_states = 9\n",
    "    for s in range(all_possible_states):\n",
    "        dict[s] = {}\n",
    "        for a in range(all_possible_states):\n",
    "            dict[s][a] = 0\n",
    "    return dict\n",
    "\n",
    "\n",
    "class TicTacToe(SingleAgentEnv):\n",
    "    def __init__(self):\n",
    "        self.cases = [-1] * 9\n",
    "        self.game_state = 0\n",
    "        self.game_over = False\n",
    "        self.player_turn = True\n",
    "        self.player_value = 1\n",
    "        self.random_player_value = 0\n",
    "        self.current_score = 0.0\n",
    "        self.reset()\n",
    "\n",
    "    def state_id(self) -> int:\n",
    "        sum = 0\n",
    "        available_actions_size = 2\n",
    "        for i in range(len(self.cases)):\n",
    "            case = self.cases[i]\n",
    "            if case == self.player_value:\n",
    "                sum += pow(available_actions_size, i)\n",
    "            elif case == self.random_player_value:\n",
    "                sum += pow(available_actions_size, len(self.cases) + i)\n",
    "        return sum\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.game_over\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        if self.cases[action_id] != -1:\n",
    "            print(self.cases)\n",
    "            print(action_id)\n",
    "            print(self.available_actions_ids())\n",
    "        assert (action_id < len(self.cases))\n",
    "        assert (self.cases[action_id] == -1)\n",
    "        assert (not self.game_over)\n",
    "\n",
    "        if self.player_turn:\n",
    "            self.cases[action_id] = self.player_value\n",
    "        else:\n",
    "            self.cases[action_id] = self.random_player_value\n",
    "\n",
    "        self.player_turn = not self.player_turn\n",
    "        self.game_state = self.state_id()\n",
    "\n",
    "        if self.tictactoe_ended(self.player_value):\n",
    "            self.game_over = True\n",
    "            self.current_score = 1.0\n",
    "        elif self.tictactoe_ended(self.random_player_value):\n",
    "            self.game_over = True\n",
    "            self.current_score = -1.0\n",
    "        elif -1 not in self.cases:\n",
    "            self.game_over = True\n",
    "            self.current_score = 0.0\n",
    "        elif not self.player_turn:\n",
    "            rand = random.randint(0, 8)\n",
    "            while self.cases[rand] != -1:\n",
    "                rand = random.randint(0, 8)\n",
    "            self.act_with_action_id(rand)\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.current_score\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        if self.game_over:\n",
    "            return np.array([], dtype=np.int)\n",
    "        available_actions = []\n",
    "        for i in range(len(self.cases)):\n",
    "            if self.cases[i] == -1:\n",
    "                available_actions.append(i)\n",
    "        return np.array(available_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.game_over = False\n",
    "        self.current_score = 0.0\n",
    "        self.game_state = 0\n",
    "        self.player_turn = True\n",
    "        self.cases = [-1] * 9\n",
    "\n",
    "    def line_checked(self, cases) -> bool:\n",
    "        return (0 in cases and 1 in cases and 2 in cases) or \\\n",
    "               (3 in cases and 4 in cases and 5 in cases) or \\\n",
    "               (6 in cases and 7 in cases and 8 in cases)\n",
    "\n",
    "    def column_checked(self, cases) -> bool:\n",
    "        return (0 in cases and 3 in cases and 6 in cases) or \\\n",
    "               (1 in cases and 4 in cases and 7 in cases) or \\\n",
    "               (2 in cases and 5 in cases and 8 in cases)\n",
    "\n",
    "    def diagonal_checked(self, cases) -> bool:\n",
    "        return (0 in cases and 4 in cases and 8 in cases) or \\\n",
    "               (2 in cases and 4 in cases and 6 in cases)\n",
    "\n",
    "    def tictactoe_ended(self, player_indice) -> bool:\n",
    "        player_indice_cases = []\n",
    "        for i, case in enumerate(self.cases):\n",
    "            if case == player_indice:\n",
    "                player_indice_cases.append(i)\n",
    "\n",
    "        if self.line_checked(player_indice_cases) or self.column_checked(player_indice_cases) or self.diagonal_checked(player_indice_cases):\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_monte_carlo_es(env) -> PolicyAndActionValueFunction:\n",
    "    max_episodes_count = 10000\n",
    "    gamma = 0.85\n",
    "\n",
    "    pi = {}\n",
    "    q = {}\n",
    "    returns = {}\n",
    "\n",
    "    for ep in tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            S.append(s)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                returns[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    returns[s][a] = []\n",
    "            chosen_action = available_actions[np.random.randint(len(available_actions))]\n",
    "            A.append(chosen_action)\n",
    "\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            G = 0\n",
    "            for t in reversed(range(len(S))):\n",
    "                G = gamma * G + R[t]\n",
    "\n",
    "                found = False\n",
    "                for prev_s, prev_a in zip(S[:t], A[:t]):\n",
    "                    if prev_s == S[t] and prev_a == A[t]:\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    continue\n",
    "\n",
    "                if A[t] not in returns[S[t]]:\n",
    "                    returns[S[t]][A[t]] = []\n",
    "\n",
    "                returns[S[t]][A[t]].append(G)\n",
    "                q[S[t]][A[t]] = np.mean(returns[S[t]][A[t]])\n",
    "                pi[S[t]] = list(q[S[t]].keys())[np.argmax(list(q[S[t]].values()))]\n",
    "\n",
    "                #max = max_dict(q[s])\n",
    "                #pi[s][max[0]] = max[1]\n",
    "\n",
    "                #optimal_a_t = list(q[S[t]].keys())[np.argmax(list(q[S[t]].values()))]\n",
    "                # pi[S[t]][optimal_a_t] = np.argmax(q[S[t]][optimal_a_t])\n",
    "                #for a_key in pi[S[t]].keys():\n",
    "                    # pi[S[t]][a_key] = np.argmax(q[S[t]][a_key])\n",
    "                    # pi[S[t]][a_key] = np.argmax(q[S[t]][optimal_a_t])\n",
    "\n",
    "    #for s in pi.keys():\n",
    "    #    probabilities = np.array(list(pi[s].values()))\n",
    "    #    probabilities /= probabilities.sum()\n",
    "    #    for i in range(len(probabilities)):\n",
    "    #        pi[s][i] = probabilities[i]\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo On Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_on_policy_monte_carlo(env) -> PolicyAndActionValueFunction:\n",
    "    epsilon = 0.1\n",
    "    max_episodes_count = 10000\n",
    "    gamma = 0.9\n",
    "\n",
    "    pi = {}\n",
    "    q = {}\n",
    "    returns = {}\n",
    "\n",
    "    for it in tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            S.append(s)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                returns[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    returns[s][a] = []\n",
    "\n",
    "            chosen_action = np.random.choice(\n",
    "                list(pi[s].keys()),\n",
    "                1,\n",
    "                False,\n",
    "                p=list(pi[s].values())\n",
    "            )[0]\n",
    "            A.append(chosen_action)\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            G = 0\n",
    "\n",
    "            for t in reversed(range(len(S))):\n",
    "                G = gamma * G + R[t]\n",
    "                s_t = S[t]\n",
    "                a_t = A[t]\n",
    "                found = False\n",
    "                for p_s, p_a in zip(S[:t], A[:t]):\n",
    "                    if s_t == p_s and a_t == p_a:\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    continue\n",
    "\n",
    "                if a_t not in returns[s_t]:\n",
    "                    returns[s_t][a_t] = []\n",
    "\n",
    "                returns[s_t][a_t].append(G)\n",
    "                q[s_t][a_t] = np.mean(returns[s_t][a_t])\n",
    "                optimal_a_t = list(q[s_t].keys())[np.argmax(list(q[s_t].values()))]\n",
    "                available_actions_t_count = len(q[s_t])\n",
    "                for a_key, q_s_a in q[s_t].items():\n",
    "                    if a_key == optimal_a_t:\n",
    "                        pi[s_t][a_key] = 1 - epsilon + epsilon / available_actions_t_count\n",
    "                    else:\n",
    "                        pi[s_t][a_key] = epsilon / available_actions_t_count\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Off Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_off_policy_monte_carlo(env) -> PolicyAndActionValueFunction:\n",
    "    max_episodes_count = 10000\n",
    "    gamma = 0.90\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    pi = {}\n",
    "\n",
    "    for it in tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            S.append(s)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                Q[s] = {}\n",
    "                C[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    Q[s][a] = 0.0\n",
    "                    C[s][a] = 0.0\n",
    "\n",
    "            chosen_action = available_actions[np.random.randint(len(available_actions))]\n",
    "\n",
    "            A.append(chosen_action)\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            G = 0\n",
    "            W = 1\n",
    "\n",
    "            for t in reversed(range(len(S))):\n",
    "                G = gamma * G + R[t]\n",
    "\n",
    "                s_t = S[t]\n",
    "                a_t = A[t]\n",
    "\n",
    "                if a_t not in C[s_t]:\n",
    "                    C[s_t][a_t] = 0.0\n",
    "\n",
    "                if a_t not in Q[s_t]:\n",
    "                    Q[s_t][a_t] = 0.0\n",
    "\n",
    "                C[s_t][a_t] += W\n",
    "                Q[s_t][a_t] += (W / (C[s_t][a_t])) * (G - Q[s_t][a_t])\n",
    "\n",
    "                max = max_dict(Q[s])\n",
    "                pi[s][max[0]] = max[1]\n",
    "                # for a_key in pi[s_t].keys():\n",
    "                #    pi[s_t][a_key] = np.argmax(Q[s_t][a_key])\n",
    "\n",
    "                optimal_a_t = list(Q[s_t].keys())[np.argmax(list(Q[s_t].values()))]\n",
    "                if chosen_action != optimal_a_t:\n",
    "                    break\n",
    "\n",
    "                W *= 1. / (available_actions[np.random.randint(len(available_actions))] + 1)\n",
    "\n",
    "    for s in pi.keys():\n",
    "        probabilities = np.array(list(pi[s].values()))\n",
    "        probabilities /= probabilities.sum()\n",
    "        for i in range(len(probabilities)):\n",
    "            pi[s][i] = probabilities[i]\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rsultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Monte Carlo ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/monte_carlo_es.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo On Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/on_policy_monte_carlo.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Off Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/off_policy_monte_carlo.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secret Envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyAndActionValueFunction(pi={0: {0: 0.32701129958531144, 1: 0.354282094842198, 2: 0.3187066055724906}, 9: {1: 1.0, 2: 1.4136344437549157e-10, 0: 0.0}, 109: {0: 0.42769230770194316, 1: 0.5723076922980568}, 128: {0: 1.0}, 39: {1: 0.9999999997225122, 2: 0.5, 0: 2.774878501529102e-10}, 153: {1: 1.0, 0: 1.0}, 19: {0: 0.3687418371943629, 2: 1.4349184880713655e-10, 1: 0.6312581628056371}, 54: {0: 0.6688128772460566, 2: 6.984919307447685e-11, 1: 0.3311871227539433}, 114: {0: 2.7855420050914686e-10, 1: 0.9999999997214457}, 63: {2: 1.0, 0: 1.0}, 118: {1: 1.0, 0: 1.0}, 93: {2: 1.0, 0: 1.0}, 133: {0: 1.0}, 163: {0: 1.0}, 58: {2: 1.0, 0: 1.0}, 123: {1: 1.0, 0: 1.0}}, q={0: {0: 1.189118460482065e-20, 1: 1.1445929154840554e-20, 2: 1.145986914951939e-20}, 9: {1: 7.273131997294813e-11, 2: 1.4161320311109802e-10}, 109: {0: 7.018826682723907e-11, 1: 1.4020089831838606e-10}, 128: {0: 1.0}, 39: {1: 1.3874392511495485e-10, 2: 6.933559606661577e-11}, 153: {1: 1.0}, 19: {0: 7.310936544641636e-11, 2: 1.4373924854641968e-10}, 54: {0: 1.4105632913802645e-10, 2: 7.581496045400076e-11}, 114: {0: 1.3927710029336967e-10, 1: 7.245125847401517e-11}, 63: {2: 1.0}, 118: {1: 1.0}, 93: {2: 1.0}, 133: {0: 1.0}, 163: {0: 1.0}, 58: {2: 1.0}, 123: {1: 1.0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolicyAndActionValueFunction(pi={0: 1, 54: 0, 93: 2, 114: 0, 9: 2, 63: 2, 163: 0, 39: 1, 123: 1, 109: 1, 118: 1, 19: 2, 58: 2, 153: 1, 133: 0, 128: 0}, q={0: {0: 0.1425663223383026, 1: 0.1461294385796545, 2: 0.14421583087512288}, 54: {0: 0.3445226917057903, 2: 0.2059401309635173}, 93: {2: 1.0}, 114: {0: 0.34268292682926826, 1: 0.2028873917228104}, 9: {1: 0.22093862815884477, 2: 0.33999999999999997}, 63: {2: 1.0}, 163: {0: 1.0}, 39: {1: 0.3344262295081967, 2: 0.22190265486725663}, 123: {1: 1.0}, 109: {0: 0.2084628670120898, 1: 0.3394988946204864}, 118: {1: 1.0}, 19: {0: 0.2125, 2: 0.33597773138482945}, 58: {2: 1.0}, 153: {1: 1.0}, 133: {0: 1.0}, 128: {0: 1.0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo On Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyAndActionValueFunction(pi={0: {0: 0.03333333333333333, 1: 0.9333333333333333, 2: 0.03333333333333333}, 109: {0: 0.05, 1: 0.9500000000000001}, 19: {0: 0.05, 2: 0.9500000000000001}, 128: {0: 1.0}, 58: {2: 1.0}, 39: {1: 0.9500000000000001, 2: 0.05}, 54: {0: 0.9500000000000001, 2: 0.05}, 163: {0: 1.0}, 153: {1: 1.0}, 114: {0: 0.9500000000000001, 1: 0.05}, 133: {0: 1.0}, 9: {1: 0.05, 2: 0.9500000000000001}, 63: {2: 1.0}, 123: {1: 1.0}, 118: {1: 1.0}, 93: {2: 1.0}}, q={0: {0: 0.19433781190019195, 1: 0.200304833319749, 2: 0.1956320657759507}, 109: {0: 0.23333333333333336, 1: 0.35932835820895526}, 19: {0: 0.225, 2: 0.36899999999999994}, 128: {0: 1.0}, 58: {2: 1.0}, 39: {1: 0.3642140468227425, 2: 0.15}, 54: {0: 0.35106382978723405, 2: 0.16363636363636364}, 163: {0: 1.0}, 153: {1: 1.0}, 114: {0: 0.3602469135802469, 1: 0.2189910979228487}, 133: {0: 1.0}, 9: {1: 0.231864406779661, 2: 0.3648286140089419}, 63: {2: 1.0}, 123: {1: 1.0}, 118: {1: 1.0}, 93: {2: 1.0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Off Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyAndActionValueFunction(pi={0: {0: 0.32701129958531144, 1: 0.354282094842198, 2: 0.3187066055724906}, 9: {1: 1.0, 2: 1.4136344437549157e-10, 0: 0.0}, 109: {0: 0.42769230770194316, 1: 0.5723076922980568}, 128: {0: 1.0}, 39: {1: 0.9999999997225122, 2: 0.5, 0: 2.774878501529102e-10}, 153: {1: 1.0, 0: 1.0}, 19: {0: 0.3687418371943629, 2: 1.4349184880713655e-10, 1: 0.6312581628056371}, 54: {0: 0.6688128772460566, 2: 6.984919307447685e-11, 1: 0.3311871227539433}, 114: {0: 2.7855420050914686e-10, 1: 0.9999999997214457}, 63: {2: 1.0, 0: 1.0}, 118: {1: 1.0, 0: 1.0}, 93: {2: 1.0, 0: 1.0}, 133: {0: 1.0}, 163: {0: 1.0}, 58: {2: 1.0, 0: 1.0}, 123: {1: 1.0, 0: 1.0}}, q={0: {0: 1.189118460482065e-20, 1: 1.1445929154840554e-20, 2: 1.145986914951939e-20}, 9: {1: 7.273131997294813e-11, 2: 1.4161320311109802e-10}, 109: {0: 7.018826682723907e-11, 1: 1.4020089831838606e-10}, 128: {0: 1.0}, 39: {1: 1.3874392511495485e-10, 2: 6.933559606661577e-11}, 153: {1: 1.0}, 19: {0: 7.310936544641636e-11, 2: 1.4373924854641968e-10}, 54: {0: 1.4105632913802645e-10, 2: 7.581496045400076e-11}, 114: {0: 1.3927710029336967e-10, 1: 7.245125847401517e-11}, 63: {2: 1.0}, 118: {1: 1.0}, 93: {2: 1.0}, 133: {0: 1.0}, 163: {0: 1.0}, 58: {2: 1.0}, 123: {1: 1.0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_best_action(epsilon, available_actions, Q, s):\n",
    "    available_actions_len = len(available_actions)\n",
    "    if available_actions_len == 1:\n",
    "        return available_actions[0]\n",
    "    elif available_actions_len == 0:\n",
    "        action_values = list(Q[s].values())\n",
    "        if len(action_values) > 0:\n",
    "            best_action_value = np.sort(action_values)[len(action_values)-1]\n",
    "            best_action = list(Q[s].keys())[list(Q[s].values()).index(best_action_value)]\n",
    "            return best_action\n",
    "        else:\n",
    "            return np.random.randint(8)\n",
    "\n",
    "    if np.random.uniform(0, 1) > epsilon:\n",
    "        return available_actions[np.random.randint(available_actions_len)]\n",
    "    else:\n",
    "        for i in range(len(list(Q[s].keys())) - 1, 0, -1):\n",
    "            best_action_value = np.sort(list(Q[s].values()))[i]\n",
    "            best_action = list(Q[s].keys())[list(Q[s].values()).index(best_action_value)]\n",
    "            if best_action in available_actions:\n",
    "                return best_action\n",
    "        return available_actions[np.random.randint(available_actions_len)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def algo_q_learning(env) -> PolicyAndActionValueFunction:\n",
    "    alpha = 0.1\n",
    "    epsilon = 1.0\n",
    "    gamma = 0.9\n",
    "    max_iter = 10000\n",
    "\n",
    "    pi = {}  # learned greedy policy\n",
    "    b = {}  # behaviour epsilon-greedy policy\n",
    "    q = {}  # action-value function of pi\n",
    "\n",
    "    for it in tqdm(range(max_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            available_actions = env.available_actions_ids()\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                b[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    b[s][a] = 1.0 / len(available_actions)\n",
    "\n",
    "            # actions disponibles differents selon les states\n",
    "            available_actions_count = len(available_actions)\n",
    "            optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "            for a_key, q_s_a in q[s].items():\n",
    "                if a_key == optimal_a:\n",
    "                    b[s][a_key] = 1 - epsilon + epsilon / available_actions_count\n",
    "                else:\n",
    "                    b[s][a_key] = epsilon / available_actions_count\n",
    "\n",
    "            chosen_action = np.random.choice(\n",
    "                list(b[s].keys()),\n",
    "                1,\n",
    "                False,\n",
    "                p=list(b[s].values())\n",
    "            )[0]\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            s_p = env.state_id()\n",
    "            next_available_actions = env.available_actions_ids()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                q[s][chosen_action] += alpha * (r + 0.0 - q[s][chosen_action])\n",
    "            else:\n",
    "                if s_p not in pi:\n",
    "                    pi[s_p] = {}\n",
    "                    q[s_p] = {}\n",
    "                    b[s_p] = {}\n",
    "                    for a in next_available_actions:\n",
    "                        pi[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                        q[s_p][a] = 0.0\n",
    "                        b[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                q[s][chosen_action] += alpha * (r + gamma * np.max(list(q[s_p].values())) - q[s][chosen_action])\n",
    "\n",
    "    for s in q.keys():\n",
    "        optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "        for a_key, q_s_a in q[s].items():\n",
    "            if a_key == optimal_a:\n",
    "                pi[s][a_key] = 1.0\n",
    "            else:\n",
    "                pi[s][a_key] = 0.0\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_sarsa(env) -> PolicyAndActionValueFunction:\n",
    "    max_episodes_count = 10000\n",
    "    alpha = 0.85\n",
    "    gamma = 0.95\n",
    "    epsilon = 0.1\n",
    "\n",
    "    Q = {}\n",
    "    pi = {}\n",
    "\n",
    "    for ep in tqdm(range(max_episodes_count)):\n",
    "\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "\n",
    "        s_1 = env.state_id()\n",
    "        available_actions = env.available_actions_ids()\n",
    "        if s_1 not in Q:\n",
    "            pi[s_1] = {}\n",
    "            Q[s_1] = {}\n",
    "            for a in available_actions:\n",
    "                pi[s_1][a] = 1.0 / len(available_actions)\n",
    "                Q[s_1][a] = 0.0\n",
    "        action_1 = get_epsilon_best_action(epsilon, available_actions, Q, s_1)\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            S.append(s_1)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s_1 not in Q:\n",
    "                pi[s_1] = {}\n",
    "                Q[s_1] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s_1][a] = 1.0 / len(available_actions)\n",
    "                    Q[s_1][a] = 0.0\n",
    "\n",
    "            A.append(action_1)\n",
    "\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(action_1)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            s_2 = env.state_id()\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s_2 not in Q:\n",
    "                Q[s_2] = {}\n",
    "                pi[s_2] = {}\n",
    "                for a in available_actions:\n",
    "                    Q[s_2][a] = 0.0\n",
    "                    pi[s_2][a] = 1.0 / len(available_actions)\n",
    "\n",
    "            action_2 = get_epsilon_best_action(epsilon, available_actions, Q, s_2)\n",
    "\n",
    "            if action_2 not in Q[s_2]:\n",
    "                Q[s_2][action_2] = 0.0\n",
    "\n",
    "            target = r + gamma * Q[s_2][action_2]\n",
    "            Q[s_1][action_1] += alpha * (target - Q[s_1][action_1])\n",
    "\n",
    "            #for a_key in pi[s_1].keys():\n",
    "            #    max = np.argmax(Q[s_1][a_key])\n",
    "            #    pi[s_1][a_key] = max\n",
    "            s_1 = s_2\n",
    "            action_1 = action_2\n",
    "\n",
    "    for s in Q.keys():\n",
    "        max = max_dict(Q[s])\n",
    "        pi[s][max[0]] = max[1]\n",
    "        probabilities = np.array(list(pi[s].values()))\n",
    "        probabilities /= probabilities.sum()\n",
    "        for i in range(len(probabilities)):\n",
    "            pi[s][i] = probabilities[i]\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_expected_sarsa(env) -> PolicyAndActionValueFunction:\n",
    "    alpha = 0.1\n",
    "    epsilon = 1.0\n",
    "    gamma = 0.9\n",
    "    max_iter = 10000\n",
    "\n",
    "    pi = {}  # learned greedy policy\n",
    "    b = {}  # behaviour epsilon-greedy policy\n",
    "    q = {}  # action-value function of pi\n",
    "\n",
    "    for it in tqdm(range(max_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            available_actions = env.available_actions_ids()\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                b[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    b[s][a] = 1.0 / len(available_actions)\n",
    "\n",
    "            # actions disponibles differents selon les states\n",
    "            available_actions_count = len(available_actions)\n",
    "            optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "            for a_key, q_s_a in q[s].items():\n",
    "                if a_key == optimal_a:\n",
    "                    b[s][a_key] = 1 - epsilon + epsilon / available_actions_count\n",
    "                else:\n",
    "                    b[s][a_key] = epsilon / available_actions_count\n",
    "\n",
    "            chosen_action = np.random.choice(\n",
    "                list(b[s].keys()),\n",
    "                1,\n",
    "                False,\n",
    "                p=list(b[s].values())\n",
    "            )[0]\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            s_p = env.state_id()\n",
    "            next_available_actions = env.available_actions_ids()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                q[s][chosen_action] += alpha * (r + 0.0 - q[s][chosen_action])\n",
    "            else:\n",
    "                if s_p not in pi:\n",
    "                    pi[s_p] = {}\n",
    "                    q[s_p] = {}\n",
    "                    b[s_p] = {}\n",
    "                    for a in next_available_actions:\n",
    "                        pi[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                        q[s_p][a] = 0.0\n",
    "                        b[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                sum = 0\n",
    "                for a in pi[s_p]:\n",
    "                    sum += pi[s_p][a] * q[s_p][a]\n",
    "                q[s][chosen_action] += alpha * (r + gamma * sum - q[s][chosen_action])\n",
    "\n",
    "    for s in q.keys():\n",
    "        optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "        for a_key, q_s_a in q[s].items():\n",
    "            if a_key == optimal_a:\n",
    "                pi[s][a_key] = 1.0\n",
    "            else:\n",
    "                pi[s][a_key] = 0.0\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/q_learning.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sarsa.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ex_sarsa.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TicTacToe Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tic_tac_toe_dict():\n",
    "    dict = {}\n",
    "    all_possible_states = 9\n",
    "    for s in range(all_possible_states):\n",
    "        dict[s] = {}\n",
    "        for a in range(all_possible_states):\n",
    "            dict[s][a] = 0\n",
    "    return dict\n",
    "\n",
    "\n",
    "class TicTacToe(DeepSingleAgentWithDiscreteActionsEnv):\n",
    "    def __init__(self):\n",
    "        self.cases = [-1] * 9\n",
    "        self.game_state = 0\n",
    "        self.game_over = False\n",
    "        self.player_turn = True\n",
    "        self.player_value = 1\n",
    "        self.random_player_value = 0\n",
    "        self.current_score = 0.0\n",
    "        self.reset()\n",
    "\n",
    "    def state_id(self) -> int:\n",
    "        sum = 0\n",
    "        available_actions_size = 2\n",
    "        for i in range(len(self.cases)):\n",
    "            case = self.cases[i]\n",
    "            if case == self.player_value:\n",
    "                sum += pow(available_actions_size, i)\n",
    "            elif case == self.random_player_value:\n",
    "                sum += pow(available_actions_size, len(self.cases) + i)\n",
    "        return sum\n",
    "\n",
    "    def state_description(self) -> int:\n",
    "        return self.cases\n",
    "\n",
    "    def state_description_length(self) -> int:\n",
    "        return 9\n",
    "\n",
    "    def max_actions_count(self) -> int:\n",
    "        return 9\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.game_over\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        if self.cases[action_id] != -1:\n",
    "            print(self.cases)\n",
    "            print(action_id)\n",
    "            print(self.available_actions_ids())\n",
    "        assert (action_id < len(self.cases))\n",
    "        assert (self.cases[action_id] == -1)\n",
    "        assert (not self.game_over)\n",
    "\n",
    "        if self.player_turn:\n",
    "            self.cases[action_id] = self.player_value\n",
    "        else:\n",
    "            self.cases[action_id] = self.random_player_value\n",
    "\n",
    "        self.player_turn = not self.player_turn\n",
    "        self.game_state = self.state_id()\n",
    "\n",
    "        if self.tictactoe_ended(self.player_value):\n",
    "            self.game_over = True\n",
    "            self.current_score = 1.0\n",
    "        elif self.tictactoe_ended(self.random_player_value):\n",
    "            self.game_over = True\n",
    "            self.current_score = -1.0\n",
    "        elif -1 not in self.cases:\n",
    "            self.game_over = True\n",
    "            self.current_score = 0.0\n",
    "        elif not self.player_turn:\n",
    "            rand = random.randint(0, 8)\n",
    "            while self.cases[rand] != -1:\n",
    "                rand = random.randint(0, 8)\n",
    "            self.act_with_action_id(rand)\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.current_score\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        if self.game_over:\n",
    "            return np.array([], dtype=np.int)\n",
    "        available_actions = []\n",
    "        for i in range(len(self.cases)):\n",
    "            if self.cases[i] == -1:\n",
    "                available_actions.append(i)\n",
    "        return np.array(available_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.game_over = False\n",
    "        self.current_score = 0.0\n",
    "        self.game_state = 0\n",
    "        self.player_turn = True\n",
    "        self.cases = [-1] * 9\n",
    "\n",
    "    def line_checked(self, cases) -> bool:\n",
    "        return (0 in cases and 1 in cases and 2 in cases) or \\\n",
    "               (3 in cases and 4 in cases and 5 in cases) or \\\n",
    "               (6 in cases and 7 in cases and 8 in cases)\n",
    "\n",
    "    def column_checked(self, cases) -> bool:\n",
    "        return (0 in cases and 3 in cases and 6 in cases) or \\\n",
    "               (1 in cases and 4 in cases and 7 in cases) or \\\n",
    "               (2 in cases and 5 in cases and 8 in cases)\n",
    "\n",
    "    def diagonal_checked(self, cases) -> bool:\n",
    "        return (0 in cases and 4 in cases and 8 in cases) or \\\n",
    "               (2 in cases and 4 in cases and 6 in cases)\n",
    "\n",
    "    def tictactoe_ended(self, player_indice) -> bool:\n",
    "        player_indice_cases = []\n",
    "        for i, case in enumerate(self.cases):\n",
    "            if case == player_indice:\n",
    "                player_indice_cases.append(i)\n",
    "\n",
    "        if self.line_checked(player_indice_cases) or self.column_checked(player_indice_cases) or self.diagonal_checked(player_indice_cases):\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacman Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wall(cases, start_x, start_y, x, y):\n",
    "    for i in range(start_x, start_x + x):\n",
    "        for j in range(start_y, start_y + y):\n",
    "            cases[i][j] = -1\n",
    "    return cases\n",
    "\n",
    "\n",
    "# -1 mur / 0 vide / 1 dot / 2 mega dot\n",
    "def initiate_map():\n",
    "    cases = []\n",
    "    for line in range(29):\n",
    "        cases.append([1] * 26)\n",
    "\n",
    "    cases[2][0] = 2\n",
    "    cases[22][0] = 2\n",
    "    cases[2][25] = 2\n",
    "    cases[22][25] = 2\n",
    "\n",
    "    for i in range(4):\n",
    "        cases[i][12] = -1\n",
    "        cases[i][13] = -1\n",
    "\n",
    "    for i in range(8, 19):\n",
    "        for j in range(6, 20):\n",
    "            cases[i][j] = 0\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        cases[13][i] = 0\n",
    "    for i in range(21, 26):\n",
    "        cases[13][i] = 0\n",
    "\n",
    "    cases = add_wall(cases, 1, 1, 3, 4)\n",
    "    cases = add_wall(cases, 1, 21, 3, 4)\n",
    "    cases = add_wall(cases, 1, 6, 3, 5)\n",
    "    cases = add_wall(cases, 1, 15, 3, 5)\n",
    "    cases = add_wall(cases, 5, 1, 2, 4)\n",
    "    cases = add_wall(cases, 5, 21, 2, 4)\n",
    "    cases = add_wall(cases, 11, 9, 5, 8)\n",
    "\n",
    "    cases = add_wall(cases, 8, 0, 5, 5)\n",
    "    cases = add_wall(cases, 14, 0, 5, 5)\n",
    "    cases = add_wall(cases, 8, 21, 5, 5)\n",
    "    cases = add_wall(cases, 14, 21, 5, 5)\n",
    "\n",
    "    cases = add_wall(cases, 5, 6, 8, 2)\n",
    "    cases = add_wall(cases, 8, 7, 2, 4)\n",
    "    cases = add_wall(cases, 5, 18, 8, 2)\n",
    "    cases = add_wall(cases, 8, 15, 2, 4)\n",
    "\n",
    "    cases = add_wall(cases, 14, 6, 5, 2)\n",
    "    cases = add_wall(cases, 14, 18, 5, 2)\n",
    "\n",
    "    cases = add_wall(cases, 5, 9, 2, 8)\n",
    "    cases = add_wall(cases, 7, 12, 3, 2)\n",
    "    cases = add_wall(cases, 17, 9, 2, 8)\n",
    "    cases = add_wall(cases, 19, 12, 3, 2)\n",
    "    cases = add_wall(cases, 23, 9, 2, 8)\n",
    "    cases = add_wall(cases, 25, 12, 3, 2)\n",
    "\n",
    "    cases = add_wall(cases, 20, 15, 2, 5)\n",
    "    cases = add_wall(cases, 20, 6, 2, 5)\n",
    "\n",
    "    cases = add_wall(cases, 20, 1, 2, 4)\n",
    "    cases = add_wall(cases, 22, 3, 3, 2)\n",
    "    cases = add_wall(cases, 20, 21, 2, 4)\n",
    "    cases = add_wall(cases, 22, 21, 3, 2)\n",
    "\n",
    "    cases = add_wall(cases, 23, 0, 2, 2)\n",
    "    cases = add_wall(cases, 23, 24, 2, 2)\n",
    "\n",
    "    cases = add_wall(cases, 26, 1, 2, 10)\n",
    "    cases = add_wall(cases, 23, 6, 3, 2)\n",
    "    cases = add_wall(cases, 26, 15, 2, 10)\n",
    "    cases = add_wall(cases, 23, 18, 3, 2)\n",
    "\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacMan(DeepSingleAgentWithDiscreteActionsEnv):\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.cases = initiate_map()\n",
    "        self.game_over = False\n",
    "        self.current_score = 0.0\n",
    "        self.round_counter = 0\n",
    "        self.move_time = TimeCapsule(0.3)\n",
    "        self.pacman_position = {'x': 13, 'y': 22}\n",
    "        self.ghost_spawn_position = [\n",
    "            [13, 10],\n",
    "            [8, 14],\n",
    "            [17, 14],\n",
    "            [17, 10]\n",
    "        ]\n",
    "        self.ghosts = [\n",
    "            {'x': self.ghost_spawn_position[i][0], 'y': self.ghost_spawn_position[i][1], 'dead': False, 'time_to_respawn': TimeCapsule(3)} for i in range(4)\n",
    "        ]\n",
    "        self.take_energizer = False\n",
    "        self.energizer_time = TimeCapsule(5.0)\n",
    "        self.reset()\n",
    "\n",
    "    def state_description(self) -> np.ndarray:\n",
    "        complete_cases = self.get_complete_cases()\n",
    "        return np.hstack(complete_cases)\n",
    "\n",
    "    def get_complete_cases(self):\n",
    "        cases_copy = np.array(self.cases)\n",
    "        for i in range(4):\n",
    "            cases_copy[self.ghosts[i]['y'], self.ghosts[i]['x']] = i + 4\n",
    "        cases_copy[self.pacman_position['y']][self.pacman_position['x']] = 3\n",
    "        return cases_copy\n",
    "\n",
    "    def state_description_length(self) -> int:\n",
    "        return 29 * 26\n",
    "\n",
    "    def max_actions_count(self) -> int:\n",
    "        return 4\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.game_over\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        assert (3 >= action_id >= 0)\n",
    "        assert (not self.game_over)\n",
    "\n",
    "        if self.take_energizer and self.energizer_time.can_execute():\n",
    "            self.take_energizer = False\n",
    "\n",
    "        x = self.pacman_position['x']\n",
    "        y = self.pacman_position['y']\n",
    "\n",
    "        if action_id == 0:\n",
    "            x -= 1\n",
    "        elif action_id == 1:\n",
    "            x += 1\n",
    "        elif action_id == 2:\n",
    "            y -= 1\n",
    "        elif action_id == 3:\n",
    "            y += 1\n",
    "\n",
    "        self.move_ghosts()\n",
    "        if 0 <= x < 26 and 0 <= y < 29:\n",
    "            if self.cases[y][x] != -1:\n",
    "                self.pacman_position['x'] = x\n",
    "                self.pacman_position['y'] = y\n",
    "\n",
    "                case_value = self.cases[y][x]\n",
    "                if case_value == 1:\n",
    "                    self.current_score += 100\n",
    "                    self.cases[y][x] = 0\n",
    "                elif case_value == 2:\n",
    "                    self.cases[y][x] = 0\n",
    "                    self.take_energizer = True\n",
    "                    self.energizer_time.restart()\n",
    "\n",
    "        if self.check_game_ended():\n",
    "            self.current_score += 10000\n",
    "            self.game_over = True\n",
    "\n",
    "        #self.current_score -= 5\n",
    "        self.round_counter += 1\n",
    "\n",
    "    def check_game_ended(self):\n",
    "        dot_counter = 0\n",
    "        for i in range(len(self.cases)):\n",
    "            for j in range(len(self.cases[i])):\n",
    "                if self.cases[i][j] == 1:\n",
    "                    dot_counter += 1\n",
    "\n",
    "        if self.round_counter % 10 == 0:\n",
    "            print(f\"Dots left : {dot_counter}\")\n",
    "            print(f\"Score : {self.current_score}\")\n",
    "        return dot_counter == 0\n",
    "\n",
    "    def move_ghosts(self):\n",
    "\n",
    "        if self.game_over:\n",
    "            return\n",
    "\n",
    "        for i in range(4):\n",
    "\n",
    "            if self.ghosts[i]['dead']:\n",
    "                if self.ghosts[i]['time_to_respawn'].can_execute():\n",
    "                    self.ghosts[i]['dead'] = False\n",
    "                    self.ghosts[i]['x'] = 13\n",
    "                    self.ghosts[i]['y'] = 10\n",
    "                continue\n",
    "\n",
    "            ghost_available_action = self.get_available_actions(i)\n",
    "            random_action = ghost_available_action[np.random.randint(len(ghost_available_action))]\n",
    "\n",
    "            new_x = self.ghosts[i]['x']\n",
    "            new_y = self.ghosts[i]['y']\n",
    "\n",
    "            if random_action == 0:\n",
    "                new_x -= 1\n",
    "            elif random_action == 1:\n",
    "                new_x += 1\n",
    "            elif random_action == 2:\n",
    "                new_y -= 1\n",
    "            elif random_action == 3:\n",
    "                new_y += 1\n",
    "\n",
    "            if 0 <= new_x < 26 and 0 <= new_y < 29:\n",
    "                if self.pacman_position['x'] == new_x and self.pacman_position['y'] == new_y:\n",
    "                    if self.take_energizer:\n",
    "                        self.current_score += 10000\n",
    "                        self.ghosts[i] = {'x': 13, 'y': 4, 'dead': True, 'time_to_respawn': TimeCapsule(3)}\n",
    "                    else:\n",
    "                        self.current_score -= 0 # 100000\n",
    "                        self.game_over = True\n",
    "                        return\n",
    "                elif self.cases[new_y][new_x] != -1:\n",
    "                    can_swap_places = True\n",
    "                    for j in range(4):\n",
    "                        if i != j:\n",
    "                            can_swap_places = self.ghosts[j]['x'] != new_x or self.ghosts[j]['y'] != new_y\n",
    "                            if not can_swap_places:\n",
    "                                continue\n",
    "\n",
    "                    if can_swap_places:\n",
    "                        self.ghosts[i]['x'] = new_x\n",
    "                        self.ghosts[i]['y'] = new_y\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.current_score\n",
    "\n",
    "    def get_available_actions(self, ghost_id):\n",
    "        if self.game_over:\n",
    "            return np.array([], dtype=np.int)\n",
    "\n",
    "        available_actions = []\n",
    "        if ghost_id is not None:\n",
    "            pos_x, pos_y = self.ghosts[ghost_id]['x'], self.ghosts[ghost_id]['y']\n",
    "        else:\n",
    "            pos_x, pos_y = self.pacman_position['x'], self.pacman_position['y']\n",
    "\n",
    "        if 0 <= (pos_x - 1) < 26 and self.cases[pos_y][pos_x - 1] != -1:\n",
    "            available_actions.append(0)\n",
    "        if 0 <= (pos_x + 1) < 26 and self.cases[pos_y][pos_x + 1] != -1:\n",
    "            available_actions.append(1)\n",
    "        if 0 <= pos_y - 1 < 29 and self.cases[pos_y - 1][pos_x] != -1:\n",
    "            available_actions.append(2)\n",
    "        if 0 <= pos_y + 1 < 29 and self.cases[pos_y + 1][pos_x] != -1:\n",
    "            available_actions.append(3)\n",
    "\n",
    "        return np.array(available_actions, dtype=np.int)\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        return self.get_available_actions(None)\n",
    "\n",
    "    def reset(self):\n",
    "        pygame.init()\n",
    "        self.cases = initiate_map()\n",
    "        self.game_state = 0\n",
    "        self.game_over = False\n",
    "        self.current_score = 0.0\n",
    "        self.move_time = TimeCapsule(0.3)\n",
    "        self.pacman_position = {'x': 13, 'y': 22}\n",
    "        self.ghost_spawn_position = [\n",
    "            [13, 10],\n",
    "            [8, 14],\n",
    "            [17, 14],\n",
    "            [17, 10]\n",
    "        ]\n",
    "        self.ghosts = [\n",
    "            {'x': self.ghost_spawn_position[i][0], 'y': self.ghost_spawn_position[i][1], 'dead': False,\n",
    "             'time_to_respawn': TimeCapsule(3)} for i in range(4)\n",
    "        ]\n",
    "        self.take_energizer = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Gradient Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_semi_gradient_sarsa(env: DeepSingleAgentWithDiscreteActionsEnv):\n",
    "    epsilon = 0.1\n",
    "    gamma = 0.9\n",
    "    max_episodes_count = 100 if not isinstance(env, PacMan) else 10\n",
    "    pre_warm = (max_episodes_count / 10) if not isinstance(env, PacMan) else 3\n",
    "\n",
    "    state_description_length = env.state_description_length()\n",
    "    max_actions_count = env.max_actions_count()\n",
    "\n",
    "    q = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.tanh,\n",
    "                              input_dim=(state_description_length + max_actions_count)),\n",
    "        tf.keras.layers.Dense(1, activation=tf.keras.activations.linear),\n",
    "    ])\n",
    "\n",
    "    q.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.mse)\n",
    "\n",
    "    for episode_id in tqdm.tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        round_counter = 0\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            round_counter += 1\n",
    "            s = env.state_description()\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if (episode_id < pre_warm) or np.random.uniform(0.0, 1.0) < epsilon:\n",
    "                chosen_action = np.random.choice(available_actions)\n",
    "            else:\n",
    "                all_q_inputs = np.zeros((len(available_actions), state_description_length + max_actions_count))\n",
    "                for i, a in enumerate(available_actions):\n",
    "                    all_q_inputs[i] = np.hstack([s, tf.keras.utils.to_categorical(a, max_actions_count)])\n",
    "\n",
    "                all_q_values = np.squeeze(q.predict(all_q_inputs))\n",
    "                chosen_action = available_actions[np.argmax(all_q_values)]\n",
    "\n",
    "            previous_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - previous_score\n",
    "            s_p = env.state_description()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                target = r\n",
    "                q_inputs = np.hstack([s, tf.keras.utils.to_categorical(chosen_action, max_actions_count)])\n",
    "                q.train_on_batch(np.array([q_inputs]), np.array([target]))\n",
    "                break\n",
    "\n",
    "            next_available_actions = env.available_actions_ids()\n",
    "\n",
    "            if episode_id < pre_warm or np.random.uniform(0.0, 1.0) < epsilon:\n",
    "                next_chosen_action = np.random.choice(next_available_actions)\n",
    "            else:\n",
    "                next_chosen_action = None\n",
    "                next_chosen_action_q_value = None\n",
    "                for a in next_available_actions:\n",
    "                    q_inputs = np.hstack([s_p, tf.keras.utils.to_categorical(a, max_actions_count)])\n",
    "                    q_value = q.predict(np.array([q_inputs]))[0][0]\n",
    "                    if next_chosen_action is None or next_chosen_action_q_value < q_value:\n",
    "                        next_chosen_action = a\n",
    "                        next_chosen_action_q_value = q_value\n",
    "\n",
    "            next_q_inputs = np.hstack([s_p, tf.keras.utils.to_categorical(next_chosen_action, max_actions_count)])\n",
    "            next_chosen_action_q_value = q.predict(np.array([next_q_inputs]))[0][0]\n",
    "\n",
    "            target = r + gamma * next_chosen_action_q_value\n",
    "\n",
    "            q_inputs = np.hstack([s, tf.keras.utils.to_categorical(chosen_action, max_actions_count)])\n",
    "            q.train_on_batch(np.array([q_inputs]), np.array([target]))\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(env: DeepSingleAgentWithDiscreteActionsEnv):\n",
    "    epsilon = 0.1\n",
    "    gamma = 0.95\n",
    "    eps_decay = 0.995\n",
    "    eps_min = 0.01\n",
    "    max_episodes_count = 100\n",
    "\n",
    "    state_description_length = env.state_description_length()\n",
    "    max_actions_count = env.max_actions_count()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation=tf.keras.activations.relu,\n",
    "                              input_dim=(state_description_length + max_actions_count)),\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(1, activation=tf.keras.activations.linear),\n",
    "    ])\n",
    "    target_model = model\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.mse)\n",
    "    target_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.mse)\n",
    "\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    buffer = ReplayBuffer()\n",
    "\n",
    "    for episode_id in tqdm.tqdm(range(max_episodes_count)):\n",
    "        done, total_reward = False, 0\n",
    "        s = env.state_description()\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            # get model action\n",
    "            epsilon *= eps_decay\n",
    "            epsilon = max(epsilon, eps_min)\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice(available_actions)\n",
    "            else:\n",
    "                all_q_inputs = get_q_inputs(available_actions, s, state_description_length, max_actions_count)\n",
    "                all_q_values = np.squeeze(model.predict(all_q_inputs))\n",
    "                action = available_actions[np.argmax(all_q_values)]\n",
    "\n",
    "            # step env\n",
    "            previous_score = env.score()\n",
    "            env.act_with_action_id(action)\n",
    "            r = env.score() - previous_score\n",
    "            next_available_action = env.available_actions_ids()\n",
    "\n",
    "            next_s = env.state_description()\n",
    "            buffer.put(s, action, available_actions, r * 0.01, next_s, next_available_action, env.is_game_over())\n",
    "            total_reward += r\n",
    "            s = next_s\n",
    "\n",
    "            if buffer.size() >= batch_size:\n",
    "                for _ in range(10):\n",
    "                    states, actions, available_actions, rewards, next_states, next_available_actions, done = buffer.sample()\n",
    "                    targets = []\n",
    "                    next_q_values = []\n",
    "                    for x in range(len(states)):\n",
    "                        all_q_inputs = get_q_inputs(available_actions[x], states[x], state_description_length,\n",
    "                                                       max_actions_count)\n",
    "                        targets.append(target_model.predict(all_q_inputs))\n",
    "\n",
    "                        next_all_q_inputs = get_q_inputs(next_available_actions[x], next_states[x], state_description_length,\n",
    "                                                        max_actions_count)\n",
    "                        if len(next_all_q_inputs) == 0:\n",
    "                            next_q_values.append([0])\n",
    "                        else:\n",
    "                            next_q_values.append(target_model.predict(next_all_q_inputs).max(axis=1))\n",
    "\n",
    "                    targets[range(batch_size), actions] = rewards + (1 - done) * next_q_values * gamma\n",
    "                    model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "            print(f\"Episode[{episode_id}] => Reward : {total_reward}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
