{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi(s,a)\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy = Dict[int, Dict[int, float]]\n",
    "\n",
    "# V(s)\n",
    "ValueFunction = Dict[int, float]\n",
    "\n",
    "# Q(s,a)\n",
    "ActionValueFunction = Dict[int, Dict[int, float]]\n",
    "\n",
    "\n",
    "# Pi(s,a) and V(s)\n",
    "@dataclass\n",
    "class PolicyAndValueFunction:\n",
    "    pi: Policy\n",
    "    v: ValueFunction\n",
    "\n",
    "\n",
    "# Pi(s,a) and Q(s,a)\n",
    "@dataclass\n",
    "class PolicyAndActionValueFunction:\n",
    "    pi: Policy\n",
    "    q: ActionValueFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDPEnv:\n",
    "    def states(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        pass\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        pass\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SingleAgentEnv:\n",
    "    def state_id(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        pass\n",
    "\n",
    "    def score(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    def reset_random(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DeepSingleAgentWithDiscreteActionsEnv:\n",
    "    def state_description(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def state_description_length(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def max_actions_count(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        pass\n",
    "\n",
    "    def score(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineWorld(MDPEnv):\n",
    "    def __init__(self, cells_count: int):\n",
    "        self.cells_count = cells_count\n",
    "        self.__states = np.arange(self.cells_count)\n",
    "        self.__actions = np.array([0, 1])\n",
    "        self.__rewards = np.array([-1, 0, 1])\n",
    "        self.probality = self.probality_setup()\n",
    "\n",
    "    def probality_setup(self):\n",
    "        p = np.zeros((len(self.__states), len(self.__actions), len(self.__states), len(self.__rewards)))\n",
    "        for s in range(1, self.cells_count - 2):\n",
    "            p[s, 1, s + 1, 1] = 1.0\n",
    "\n",
    "        for s in range(2, self.cells_count - 1):\n",
    "            p[s, 0, s - 1, 1] = 1.0\n",
    "\n",
    "        p[self.cells_count - 2, 1, self.cells_count - 1, 2] = 1.0\n",
    "        p[1, 0, 0, 0] = 1.0\n",
    "        return p\n",
    "\n",
    "    def states(self) -> np.ndarray:\n",
    "        return self.__states\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        return self.__actions\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        return self.__rewards\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        return s == self.cells_count - 1 or s == 0\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        return self.probality[s, a, s_p, r]\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GridWorld(MDPEnv):\n",
    "    def __init__(self, lines: int, columns: int):\n",
    "        self.lines = lines\n",
    "        self.columns = columns\n",
    "        self.cells_count = lines * columns\n",
    "        self.negative_terminal = 4\n",
    "        self.positive_terminal = self.cells_count - 1\n",
    "        self.__states = np.arange(self.cells_count)\n",
    "        self.__actions = np.array([0, 1, 2, 3])\n",
    "        self.__rewards = np.array([-1, 0, 1])\n",
    "        self.probality = self.probality_setup()\n",
    "\n",
    "    def probality_setup(self):\n",
    "        p = np.zeros((len(self.__states), len(self.__actions), len(self.__states), len(self.__rewards)))\n",
    "        for line in range(0, self.lines):\n",
    "            for column in range(0, self.columns - 1):\n",
    "                s = line * self.columns + column\n",
    "                if s != self.negative_terminal and s != self.positive_terminal:\n",
    "                    if s + 1 == self.positive_terminal:\n",
    "                        p[s, 1, s + 1, 2] = 1.0\n",
    "                    elif s + 1 == self.negative_terminal:\n",
    "                        p[s, 1, s + 1, 0] = 1.0\n",
    "                    else:\n",
    "                        p[s, 1, s + 1, 1] = 1.0\n",
    "\n",
    "            for column in range(1, self.columns):\n",
    "                s = line * self.columns + column\n",
    "                if s != self.positive_terminal and s != self.negative_terminal:\n",
    "                    if s - 1 == self.negative_terminal:\n",
    "                        p[s, 0, s - 1, 0] = 1.0\n",
    "                    elif s - 1 == self.positive_terminal:\n",
    "                        p[s, 0, s - 1, 2] = 1.0\n",
    "                    else:\n",
    "                        p[s, 0, s - 1, 1] = 1.0\n",
    "\n",
    "        for column in range(0, self.columns):\n",
    "            for line in range(0, self.lines - 1):\n",
    "                s = self.columns * line + column\n",
    "                s2 = self.columns * (line + 1) + column\n",
    "                # up\n",
    "                if s2 != self.positive_terminal and s2 != self.negative_terminal:\n",
    "                    if s == self.negative_terminal:\n",
    "                        p[s2, 2, s, 0] = 1.0\n",
    "                    elif s == self.positive_terminal:\n",
    "                        p[s2, 2, s, 2] = 1.0\n",
    "                    else:\n",
    "                        p[s2, 2, s, 1] = 1.0\n",
    "\n",
    "                # down\n",
    "                if s != self.negative_terminal and s != self.positive_terminal:\n",
    "                    if s2 == self.positive_terminal:\n",
    "                        p[s, 3, s2, 2] = 1.0\n",
    "                    elif s2 == self.negative_terminal:\n",
    "                        p[s, 3, s2, 0] = 1.0\n",
    "                    else:\n",
    "                        p[s, 3, s2, 1] = 1.0\n",
    "        return p\n",
    "\n",
    "    def states(self) -> np.ndarray:\n",
    "        return self.__states\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        return self.__actions\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        return self.__rewards\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        return s == self.positive_terminal or s == self.negative_terminal\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        return self.probality[s, a, s_p, r]\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDPEnv:\n",
    "    def states(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def actions(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def rewards(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def is_state_terminal(self, s: int) -> bool:\n",
    "        pass\n",
    "\n",
    "    def transition_probability(self, s: int, a: int, s_p: int, r: float) -> float:\n",
    "        pass\n",
    "\n",
    "    def view_state(self, s: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SingleAgentEnv:\n",
    "    def state_id(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        pass\n",
    "\n",
    "    def score(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    def reset_random(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DeepSingleAgentWithDiscreteActionsEnv:\n",
    "    def state_description(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def state_description_length(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def max_actions_count(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        pass\n",
    "\n",
    "    def act_with_action_id(self, action_id: int):\n",
    "        pass\n",
    "\n",
    "    def score(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def available_actions_ids(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation_on_line_world() -> ValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Line World of 7 cells (leftmost and rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Evaluation Algorithm in order to find the Value Function of a uniform random policy\n",
    "    Returns the Value function (V(s)) of this policy\n",
    "    \"\"\"\n",
    "    env = LineWorld(7)\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            for a in env.actions():\n",
    "                for s_next in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return dict(enumerate(V.flatten(), 1))\n",
    "\n",
    "\n",
    "def policy_iteration_on_line_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Line World of 7 cells (leftmost and rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = LineWorld(7)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in env.states():\n",
    "                old_v = V[s]\n",
    "                V[s] = 0.0\n",
    "                for a in env.actions():\n",
    "                    for s_next in env.states():\n",
    "                        for r_idx, r in enumerate(env.rewards()):\n",
    "                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in env.states():\n",
    "            old_policy = pi[s, :]\n",
    "\n",
    "            best_a = None\n",
    "            best_a_value = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            pi[s, :] = 0.0\n",
    "            pi[s, best_a] = 1.0\n",
    "            if not np.array_equal(pi[s], old_policy):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def value_iteration_on_line_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Line World of 7 cells (leftmost and rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Value Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = LineWorld(7)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "    pi2 = pi.copy()\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            best_a_value = None\n",
    "            best_a = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                        a_value += pre_a_value\n",
    "                        V[s] += pi[s, a] * pre_a_value\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "            pi2[s, :] = 0.0\n",
    "            pi2[s, best_a] = 1.0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi2):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def policy_evaluation_on_grid_world() -> ValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Grid World of 5x5 cells (upper rightmost and lower rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Evaluation Algorithm in order to find the Value Function of a uniform random policy\n",
    "    Returns the Value function (V(s)) of this policy\n",
    "    \"\"\"\n",
    "    env = GridWorld(5, 5)\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            for a in env.actions():\n",
    "                for s_next in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return dict(enumerate(V.flatten(), 1))\n",
    "\n",
    "\n",
    "def policy_iteration_on_grid_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Grid World of 5x5 cells (upper rightmost and lower rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Policy Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = GridWorld(5, 5)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in env.states():\n",
    "                old_v = V[s]\n",
    "                V[s] = 0.0\n",
    "                for a in env.actions():\n",
    "                    for s_next in env.states():\n",
    "                        for r_idx, r in enumerate(env.rewards()):\n",
    "                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in env.states():\n",
    "            old_policy = pi[s, :]\n",
    "\n",
    "            best_a = None\n",
    "            best_a_value = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            pi[s, :] = 0.0\n",
    "            pi[s, best_a] = 1.0\n",
    "            if not np.array_equal(pi[s], old_policy):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def value_iteration_on_grid_world() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Grid World of 5x5 cells (upper rightmost and lower rightmost are terminal, with -1 and 1 reward respectively)\n",
    "    Launches a Value Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = GridWorld(5, 5)\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "    pi2 = pi.copy()\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            best_a_value = None\n",
    "            best_a = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                        a_value += pre_a_value\n",
    "                        V[s] += pi[s, a] * pre_a_value\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "            pi2[s, :] = 0.0\n",
    "            pi2[s, best_a] = 1.0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi2):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def policy_evaluation_on_secret_env1() -> ValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Secret Env1\n",
    "    Launches a Policy Evaluation Algorithm in order to find the Value Function of a uniform random policy\n",
    "    Returns the Value function (V(s)) of this policy\n",
    "    \"\"\"\n",
    "    env = Env1()\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            for a in env.actions():\n",
    "                for s_next in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return dict(enumerate(V.flatten(), 1))\n",
    "\n",
    "\n",
    "def policy_iteration_on_secret_env1() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Secret Env1\n",
    "    Launches a Policy Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Returns the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = Env1()\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in env.states():\n",
    "                old_v = V[s]\n",
    "                V[s] = 0.0\n",
    "                for a in env.actions():\n",
    "                    for s_next in env.states():\n",
    "                        for r_idx, r in enumerate(env.rewards()):\n",
    "                            V[s] += pi[s, a] * env.transition_probability(s, a, s_next, r_idx) * (r + gamma * V[s_next])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in env.states():\n",
    "            old_policy = pi[s, :]\n",
    "\n",
    "            best_a = None\n",
    "            best_a_value = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        a_value += env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            pi[s, :] = 0.0\n",
    "            pi[s, best_a] = 1.0\n",
    "            if not np.array_equal(pi[s], old_policy):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n",
    "\n",
    "def value_iteration_on_secret_env1() -> PolicyAndValueFunction:\n",
    "    \"\"\"\n",
    "    Creates a Secret Env1\n",
    "    Launches a Value Iteration Algorithm in order to find the Optimal Policy and its Value Function\n",
    "    Prints the Policy (Pi(s,a)) and its Value Function (V(s))\n",
    "    \"\"\"\n",
    "    env = Env1()\n",
    "    V = np.zeros((len(env.states()),))\n",
    "    pi = np.ones((len(env.states()), len(env.actions())))\n",
    "    pi /= len(env.actions())\n",
    "    pi2 = pi.copy()\n",
    "\n",
    "    theta = 0.00001\n",
    "    gamma = 1.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states():\n",
    "            old_v = V[s]\n",
    "            V[s] = 0.0\n",
    "            best_a_value = None\n",
    "            best_a = None\n",
    "            for a in env.actions():\n",
    "                a_value = 0\n",
    "                for s_p in env.states():\n",
    "                    for r_idx, r in enumerate(env.rewards()):\n",
    "                        pre_a_value = env.transition_probability(s, a, s_p, r_idx) * (r + gamma * V[s_p])\n",
    "                        a_value += pre_a_value\n",
    "                        V[s] += pi[s, a] * pre_a_value\n",
    "                if best_a_value is None or best_a_value < a_value:\n",
    "                    best_a_value = a_value\n",
    "                    best_a = a\n",
    "\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "            pi2[s, :] = 0.0\n",
    "            pi2[s, best_a] = 1.0\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_pi = {}\n",
    "    for indice, value in enumerate(pi2):\n",
    "        final_pi[indice] = dict(enumerate(value.flatten(), 1))\n",
    "\n",
    "    return PolicyAndValueFunction(final_pi, dict(enumerate(V.flatten(), 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result fort Dynamic programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.0, 2: -0.666686199082779, 3: -0.3333626319575018, 4: -2.9298624168505594e-05, 5: 0.33331135936520695, 6: 0.6666556796826035, 7: 0.0}\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0}, 1: {1: 0.0, 2: 1.0}, 2: {1: 0.0, 2: 1.0}, 3: {1: 0.0, 2: 1.0}, 4: {1: 0.0, 2: 1.0}, 5: {1: 0.0, 2: 1.0}, 6: {1: 1.0, 2: 0.0}}, v={1: 0.0, 2: -0.666686199082779, 3: -0.3333626319575018, 4: -2.9298624168505594e-05, 5: 0.33331135936520695, 6: 0.6666556796826035, 7: 0.0})\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0}, 1: {1: 0.0, 2: 1.0}, 2: {1: 0.0, 2: 1.0}, 3: {1: 0.0, 2: 1.0}, 4: {1: 0.0, 2: 1.0}, 5: {1: 0.0, 2: 1.0}, 6: {1: 1.0, 2: 0.0}}, v={1: 0.0, 2: -0.666686199082779, 3: -0.3333626319575018, 4: -2.9298624168505594e-05, 5: 0.33331135936520695, 6: 0.6666556796826035, 7: 0.0})\n",
      "{1: -0.012408241005487585, 2: -0.03849208733039875, 3: -0.10946152767210533, 4: -0.3206577559932595, 5: 0.0, 6: -0.011130448543747447, 7: -0.032086147618454595, 8: -0.07868611730403527, 9: -0.1731645441465791, 10: -0.2932932689068334, 11: -1.445875184277138e-05, 12: -2.1020462169746984e-05, 13: -2.0020788780394233e-05, 14: -1.3950568206126668e-05, 15: -6.257202351436786e-06, 16: 0.011105162750401581, 17: 0.03204946364618418, 18: 0.07865141280595922, 19: 0.17314092624874877, 20: 0.2932836672615993, 21: 0.012393199830108002, 22: 0.03847039442485897, 23: 0.10944146483060663, 24: 0.3206455977698388, 25: 0.0}\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 1: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 2: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 3: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 4: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 5: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 6: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 7: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 8: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 9: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 10: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 11: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 12: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 13: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 14: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 15: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 16: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 17: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 18: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 19: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 20: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 21: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 22: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 23: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 24: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}}, v={1: -0.012408241005487585, 2: -0.03849208733039875, 3: -0.10946152767210533, 4: -0.3206577559932595, 5: 0.0, 6: -0.011130448543747447, 7: -0.032086147618454595, 8: -0.07868611730403527, 9: -0.1731645441465791, 10: -0.2932932689068334, 11: -1.445875184277138e-05, 12: -2.1020462169746984e-05, 13: -2.0020788780394233e-05, 14: -1.3950568206126668e-05, 15: -6.257202351436786e-06, 16: 0.011105162750401581, 17: 0.03204946364618418, 18: 0.07865141280595922, 19: 0.17314092624874877, 20: 0.2932836672615993, 21: 0.012393199830108002, 22: 0.03847039442485897, 23: 0.10944146483060663, 24: 0.3206455977698388, 25: 0.0})\n",
      "PolicyAndValueFunction(pi={0: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 1: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 2: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 3: {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}, 4: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 5: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}, 6: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 7: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 8: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 9: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 10: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 11: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 12: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 13: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 14: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 15: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 16: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 17: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 18: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 19: {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0}, 20: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 21: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 22: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 23: {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}, 24: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}}, v={1: -0.012408241005487585, 2: -0.03849208733039875, 3: -0.10946152767210533, 4: -0.3206577559932595, 5: 0.0, 6: -0.011130448543747447, 7: -0.032086147618454595, 8: -0.07868611730403527, 9: -0.1731645441465791, 10: -0.2932932689068334, 11: -1.445875184277138e-05, 12: -2.1020462169746984e-05, 13: -2.0020788780394233e-05, 14: -1.3950568206126668e-05, 15: -6.257202351436786e-06, 16: 0.011105162750401581, 17: 0.03204946364618418, 18: 0.07865141280595922, 19: 0.17314092624874877, 20: 0.2932836672615993, 21: 0.012393199830108002, 22: 0.03847039442485897, 23: 0.10944146483060663, 24: 0.3206455977698388, 25: 0.0})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Env1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19260/1073501247.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue_iteration_on_grid_world\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpolicy_evaluation_on_secret_env1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpolicy_iteration_on_secret_env1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue_iteration_on_secret_env1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19260/1909702818.py\u001B[0m in \u001B[0;36mpolicy_evaluation_on_secret_env1\u001B[1;34m()\u001B[0m\n\u001B[0;32m    279\u001B[0m     \u001B[0mReturns\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mValue\u001B[0m \u001B[0mfunction\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mV\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    280\u001B[0m     \"\"\"\n\u001B[1;32m--> 281\u001B[1;33m     \u001B[0menv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mEnv1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    282\u001B[0m     \u001B[0mpi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstates\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mactions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    283\u001B[0m     \u001B[0mpi\u001B[0m \u001B[1;33m/=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mactions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Env1' is not defined"
     ]
    }
   ],
   "source": [
    "    print(policy_evaluation_on_line_world())\n",
    "    print(policy_iteration_on_line_world())\n",
    "    print(value_iteration_on_line_world())\n",
    "\n",
    "    print(policy_evaluation_on_grid_world())\n",
    "    print(policy_iteration_on_grid_world())\n",
    "    print(value_iteration_on_grid_world())\n",
    "\n",
    "    print(policy_evaluation_on_secret_env1())\n",
    "    print(policy_iteration_on_secret_env1())\n",
    "    print(value_iteration_on_secret_env1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_monte_carlo_es(env) -> PolicyAndActionValueFunction:\n",
    "    max_episodes_count = 10000\n",
    "    gamma = 0.85\n",
    "\n",
    "    pi = {}\n",
    "    q = {}\n",
    "    returns = {}\n",
    "\n",
    "    for ep in tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            S.append(s)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                returns[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    returns[s][a] = []\n",
    "            chosen_action = available_actions[np.random.randint(len(available_actions))]\n",
    "            A.append(chosen_action)\n",
    "\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            G = 0\n",
    "            for t in reversed(range(len(S))):\n",
    "                G = gamma * G + R[t]\n",
    "\n",
    "                found = False\n",
    "                for prev_s, prev_a in zip(S[:t], A[:t]):\n",
    "                    if prev_s == S[t] and prev_a == A[t]:\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    continue\n",
    "\n",
    "                if A[t] not in returns[S[t]]:\n",
    "                    returns[S[t]][A[t]] = []\n",
    "\n",
    "                returns[S[t]][A[t]].append(G)\n",
    "                q[S[t]][A[t]] = np.mean(returns[S[t]][A[t]])\n",
    "                pi[S[t]] = list(q[S[t]].keys())[np.argmax(list(q[S[t]].values()))]\n",
    "\n",
    "                #max = max_dict(q[s])\n",
    "                #pi[s][max[0]] = max[1]\n",
    "\n",
    "                #optimal_a_t = list(q[S[t]].keys())[np.argmax(list(q[S[t]].values()))]\n",
    "                # pi[S[t]][optimal_a_t] = np.argmax(q[S[t]][optimal_a_t])\n",
    "                #for a_key in pi[S[t]].keys():\n",
    "                    # pi[S[t]][a_key] = np.argmax(q[S[t]][a_key])\n",
    "                    # pi[S[t]][a_key] = np.argmax(q[S[t]][optimal_a_t])\n",
    "\n",
    "    #for s in pi.keys():\n",
    "    #    probabilities = np.array(list(pi[s].values()))\n",
    "    #    probabilities /= probabilities.sum()\n",
    "    #    for i in range(len(probabilities)):\n",
    "    #        pi[s][i] = probabilities[i]\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo On Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_on_policy_monte_carlo(env) -> PolicyAndActionValueFunction:\n",
    "    epsilon = 0.1\n",
    "    max_episodes_count = 10000\n",
    "    gamma = 0.9\n",
    "\n",
    "    pi = {}\n",
    "    q = {}\n",
    "    returns = {}\n",
    "\n",
    "    for it in tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            S.append(s)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                returns[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    returns[s][a] = []\n",
    "\n",
    "            chosen_action = np.random.choice(\n",
    "                list(pi[s].keys()),\n",
    "                1,\n",
    "                False,\n",
    "                p=list(pi[s].values())\n",
    "            )[0]\n",
    "            A.append(chosen_action)\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            G = 0\n",
    "\n",
    "            for t in reversed(range(len(S))):\n",
    "                G = gamma * G + R[t]\n",
    "                s_t = S[t]\n",
    "                a_t = A[t]\n",
    "                found = False\n",
    "                for p_s, p_a in zip(S[:t], A[:t]):\n",
    "                    if s_t == p_s and a_t == p_a:\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    continue\n",
    "\n",
    "                if a_t not in returns[s_t]:\n",
    "                    returns[s_t][a_t] = []\n",
    "\n",
    "                returns[s_t][a_t].append(G)\n",
    "                q[s_t][a_t] = np.mean(returns[s_t][a_t])\n",
    "                optimal_a_t = list(q[s_t].keys())[np.argmax(list(q[s_t].values()))]\n",
    "                available_actions_t_count = len(q[s_t])\n",
    "                for a_key, q_s_a in q[s_t].items():\n",
    "                    if a_key == optimal_a_t:\n",
    "                        pi[s_t][a_key] = 1 - epsilon + epsilon / available_actions_t_count\n",
    "                    else:\n",
    "                        pi[s_t][a_key] = epsilon / available_actions_t_count\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Off Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_off_policy_monte_carlo(env) -> PolicyAndActionValueFunction:\n",
    "    max_episodes_count = 10000\n",
    "    gamma = 0.90\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    pi = {}\n",
    "\n",
    "    for it in tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            S.append(s)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                Q[s] = {}\n",
    "                C[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    Q[s][a] = 0.0\n",
    "                    C[s][a] = 0.0\n",
    "\n",
    "            chosen_action = available_actions[np.random.randint(len(available_actions))]\n",
    "\n",
    "            A.append(chosen_action)\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            G = 0\n",
    "            W = 1\n",
    "\n",
    "            for t in reversed(range(len(S))):\n",
    "                G = gamma * G + R[t]\n",
    "\n",
    "                s_t = S[t]\n",
    "                a_t = A[t]\n",
    "\n",
    "                if a_t not in C[s_t]:\n",
    "                    C[s_t][a_t] = 0.0\n",
    "\n",
    "                if a_t not in Q[s_t]:\n",
    "                    Q[s_t][a_t] = 0.0\n",
    "\n",
    "                C[s_t][a_t] += W\n",
    "                Q[s_t][a_t] += (W / (C[s_t][a_t])) * (G - Q[s_t][a_t])\n",
    "\n",
    "                max = max_dict(Q[s])\n",
    "                pi[s][max[0]] = max[1]\n",
    "                # for a_key in pi[s_t].keys():\n",
    "                #    pi[s_t][a_key] = np.argmax(Q[s_t][a_key])\n",
    "\n",
    "                optimal_a_t = list(Q[s_t].keys())[np.argmax(list(Q[s_t].values()))]\n",
    "                if chosen_action != optimal_a_t:\n",
    "                    break\n",
    "\n",
    "                W *= 1. / (available_actions[np.random.randint(len(available_actions))] + 1)\n",
    "\n",
    "    for s in pi.keys():\n",
    "        probabilities = np.array(list(pi[s].values()))\n",
    "        probabilities /= probabilities.sum()\n",
    "        for i in range(len(probabilities)):\n",
    "            pi[s][i] = probabilities[i]\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Monte Carlo ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/monte_carlo_es.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo On Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/on_policy_monte_carlo.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Off Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/off_policy_monte_carlo.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_best_action(epsilon, available_actions, Q, s):\n",
    "    available_actions_len = len(available_actions)\n",
    "    if available_actions_len == 1:\n",
    "        return available_actions[0]\n",
    "    elif available_actions_len == 0:\n",
    "        action_values = list(Q[s].values())\n",
    "        if len(action_values) > 0:\n",
    "            best_action_value = np.sort(action_values)[len(action_values)-1]\n",
    "            best_action = list(Q[s].keys())[list(Q[s].values()).index(best_action_value)]\n",
    "            return best_action\n",
    "        else:\n",
    "            return np.random.randint(8)\n",
    "\n",
    "    if np.random.uniform(0, 1) > epsilon:\n",
    "        return available_actions[np.random.randint(available_actions_len)]\n",
    "    else:\n",
    "        for i in range(len(list(Q[s].keys())) - 1, 0, -1):\n",
    "            best_action_value = np.sort(list(Q[s].values()))[i]\n",
    "            best_action = list(Q[s].keys())[list(Q[s].values()).index(best_action_value)]\n",
    "            if best_action in available_actions:\n",
    "                return best_action\n",
    "        return available_actions[np.random.randint(available_actions_len)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def algo_q_learning(env) -> PolicyAndActionValueFunction:\n",
    "    alpha = 0.1\n",
    "    epsilon = 1.0\n",
    "    gamma = 0.9\n",
    "    max_iter = 10000\n",
    "\n",
    "    pi = {}  # learned greedy policy\n",
    "    b = {}  # behaviour epsilon-greedy policy\n",
    "    q = {}  # action-value function of pi\n",
    "\n",
    "    for it in tqdm(range(max_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            available_actions = env.available_actions_ids()\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                b[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    b[s][a] = 1.0 / len(available_actions)\n",
    "\n",
    "            # actions disponibles differents selon les states\n",
    "            available_actions_count = len(available_actions)\n",
    "            optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "            for a_key, q_s_a in q[s].items():\n",
    "                if a_key == optimal_a:\n",
    "                    b[s][a_key] = 1 - epsilon + epsilon / available_actions_count\n",
    "                else:\n",
    "                    b[s][a_key] = epsilon / available_actions_count\n",
    "\n",
    "            chosen_action = np.random.choice(\n",
    "                list(b[s].keys()),\n",
    "                1,\n",
    "                False,\n",
    "                p=list(b[s].values())\n",
    "            )[0]\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            s_p = env.state_id()\n",
    "            next_available_actions = env.available_actions_ids()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                q[s][chosen_action] += alpha * (r + 0.0 - q[s][chosen_action])\n",
    "            else:\n",
    "                if s_p not in pi:\n",
    "                    pi[s_p] = {}\n",
    "                    q[s_p] = {}\n",
    "                    b[s_p] = {}\n",
    "                    for a in next_available_actions:\n",
    "                        pi[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                        q[s_p][a] = 0.0\n",
    "                        b[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                q[s][chosen_action] += alpha * (r + gamma * np.max(list(q[s_p].values())) - q[s][chosen_action])\n",
    "\n",
    "    for s in q.keys():\n",
    "        optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "        for a_key, q_s_a in q[s].items():\n",
    "            if a_key == optimal_a:\n",
    "                pi[s][a_key] = 1.0\n",
    "            else:\n",
    "                pi[s][a_key] = 0.0\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_sarsa(env) -> PolicyAndActionValueFunction:\n",
    "    max_episodes_count = 10000\n",
    "    alpha = 0.85\n",
    "    gamma = 0.95\n",
    "    epsilon = 0.1\n",
    "\n",
    "    Q = {}\n",
    "    pi = {}\n",
    "\n",
    "    for ep in tqdm(range(max_episodes_count)):\n",
    "\n",
    "        env.reset()\n",
    "        S = []\n",
    "        A = []\n",
    "        R = []\n",
    "\n",
    "        s_1 = env.state_id()\n",
    "        available_actions = env.available_actions_ids()\n",
    "        if s_1 not in Q:\n",
    "            pi[s_1] = {}\n",
    "            Q[s_1] = {}\n",
    "            for a in available_actions:\n",
    "                pi[s_1][a] = 1.0 / len(available_actions)\n",
    "                Q[s_1][a] = 0.0\n",
    "        action_1 = get_epsilon_best_action(epsilon, available_actions, Q, s_1)\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            S.append(s_1)\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s_1 not in Q:\n",
    "                pi[s_1] = {}\n",
    "                Q[s_1] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s_1][a] = 1.0 / len(available_actions)\n",
    "                    Q[s_1][a] = 0.0\n",
    "\n",
    "            A.append(action_1)\n",
    "\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(action_1)\n",
    "            r = env.score() - old_score\n",
    "            R.append(r)\n",
    "\n",
    "            s_2 = env.state_id()\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if s_2 not in Q:\n",
    "                Q[s_2] = {}\n",
    "                pi[s_2] = {}\n",
    "                for a in available_actions:\n",
    "                    Q[s_2][a] = 0.0\n",
    "                    pi[s_2][a] = 1.0 / len(available_actions)\n",
    "\n",
    "            action_2 = get_epsilon_best_action(epsilon, available_actions, Q, s_2)\n",
    "\n",
    "            if action_2 not in Q[s_2]:\n",
    "                Q[s_2][action_2] = 0.0\n",
    "\n",
    "            target = r + gamma * Q[s_2][action_2]\n",
    "            Q[s_1][action_1] += alpha * (target - Q[s_1][action_1])\n",
    "\n",
    "            #for a_key in pi[s_1].keys():\n",
    "            #    max = np.argmax(Q[s_1][a_key])\n",
    "            #    pi[s_1][a_key] = max\n",
    "            s_1 = s_2\n",
    "            action_1 = action_2\n",
    "\n",
    "    for s in Q.keys():\n",
    "        max = max_dict(Q[s])\n",
    "        pi[s][max[0]] = max[1]\n",
    "        probabilities = np.array(list(pi[s].values()))\n",
    "        probabilities /= probabilities.sum()\n",
    "        for i in range(len(probabilities)):\n",
    "            pi[s][i] = probabilities[i]\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_expected_sarsa(env) -> PolicyAndActionValueFunction:\n",
    "    alpha = 0.1\n",
    "    epsilon = 1.0\n",
    "    gamma = 0.9\n",
    "    max_iter = 10000\n",
    "\n",
    "    pi = {}  # learned greedy policy\n",
    "    b = {}  # behaviour epsilon-greedy policy\n",
    "    q = {}  # action-value function of pi\n",
    "\n",
    "    for it in tqdm(range(max_iter)):\n",
    "        env.reset()\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            s = env.state_id()\n",
    "            available_actions = env.available_actions_ids()\n",
    "            if s not in pi:\n",
    "                pi[s] = {}\n",
    "                q[s] = {}\n",
    "                b[s] = {}\n",
    "                for a in available_actions:\n",
    "                    pi[s][a] = 1.0 / len(available_actions)\n",
    "                    q[s][a] = 0.0\n",
    "                    b[s][a] = 1.0 / len(available_actions)\n",
    "\n",
    "            # actions disponibles differents selon les states\n",
    "            available_actions_count = len(available_actions)\n",
    "            optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "            for a_key, q_s_a in q[s].items():\n",
    "                if a_key == optimal_a:\n",
    "                    b[s][a_key] = 1 - epsilon + epsilon / available_actions_count\n",
    "                else:\n",
    "                    b[s][a_key] = epsilon / available_actions_count\n",
    "\n",
    "            chosen_action = np.random.choice(\n",
    "                list(b[s].keys()),\n",
    "                1,\n",
    "                False,\n",
    "                p=list(b[s].values())\n",
    "            )[0]\n",
    "            old_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - old_score\n",
    "            s_p = env.state_id()\n",
    "            next_available_actions = env.available_actions_ids()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                q[s][chosen_action] += alpha * (r + 0.0 - q[s][chosen_action])\n",
    "            else:\n",
    "                if s_p not in pi:\n",
    "                    pi[s_p] = {}\n",
    "                    q[s_p] = {}\n",
    "                    b[s_p] = {}\n",
    "                    for a in next_available_actions:\n",
    "                        pi[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                        q[s_p][a] = 0.0\n",
    "                        b[s_p][a] = 1.0 / len(next_available_actions)\n",
    "                sum = 0\n",
    "                for a in pi[s_p]:\n",
    "                    sum += pi[s_p][a] * q[s_p][a]\n",
    "                q[s][chosen_action] += alpha * (r + gamma * sum - q[s][chosen_action])\n",
    "\n",
    "    for s in q.keys():\n",
    "        optimal_a = list(q[s].keys())[np.argmax(list(q[s].values()))]\n",
    "        for a_key, q_s_a in q[s].items():\n",
    "            if a_key == optimal_a:\n",
    "                pi[s][a_key] = 1.0\n",
    "            else:\n",
    "                pi[s][a_key] = 0.0\n",
    "\n",
    "    return PolicyAndActionValueFunction(pi, q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/q_learning.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sarsa.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ex_sarsa.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_semi_gradient_sarsa(env: DeepSingleAgentWithDiscreteActionsEnv):\n",
    "    epsilon = 0.1\n",
    "    gamma = 0.9\n",
    "    max_episodes_count = 100 if not isinstance(env, PacMan) else 10\n",
    "    pre_warm = (max_episodes_count / 10) if not isinstance(env, PacMan) else 3\n",
    "\n",
    "    state_description_length = env.state_description_length()\n",
    "    max_actions_count = env.max_actions_count()\n",
    "\n",
    "    q = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.tanh,\n",
    "                              input_dim=(state_description_length + max_actions_count)),\n",
    "        tf.keras.layers.Dense(1, activation=tf.keras.activations.linear),\n",
    "    ])\n",
    "\n",
    "    q.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.mse)\n",
    "\n",
    "    for episode_id in tqdm.tqdm(range(max_episodes_count)):\n",
    "        env.reset()\n",
    "        round_counter = 0\n",
    "\n",
    "        while not env.is_game_over():\n",
    "            round_counter += 1\n",
    "            s = env.state_description()\n",
    "            available_actions = env.available_actions_ids()\n",
    "\n",
    "            if (episode_id < pre_warm) or np.random.uniform(0.0, 1.0) < epsilon:\n",
    "                chosen_action = np.random.choice(available_actions)\n",
    "            else:\n",
    "                all_q_inputs = np.zeros((len(available_actions), state_description_length + max_actions_count))\n",
    "                for i, a in enumerate(available_actions):\n",
    "                    all_q_inputs[i] = np.hstack([s, tf.keras.utils.to_categorical(a, max_actions_count)])\n",
    "\n",
    "                all_q_values = np.squeeze(q.predict(all_q_inputs))\n",
    "                chosen_action = available_actions[np.argmax(all_q_values)]\n",
    "\n",
    "            previous_score = env.score()\n",
    "            env.act_with_action_id(chosen_action)\n",
    "            r = env.score() - previous_score\n",
    "            s_p = env.state_description()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                target = r\n",
    "                q_inputs = np.hstack([s, tf.keras.utils.to_categorical(chosen_action, max_actions_count)])\n",
    "                q.train_on_batch(np.array([q_inputs]), np.array([target]))\n",
    "                break\n",
    "\n",
    "            next_available_actions = env.available_actions_ids()\n",
    "\n",
    "            if episode_id < pre_warm or np.random.uniform(0.0, 1.0) < epsilon:\n",
    "                next_chosen_action = np.random.choice(next_available_actions)\n",
    "            else:\n",
    "                next_chosen_action = None\n",
    "                next_chosen_action_q_value = None\n",
    "                for a in next_available_actions:\n",
    "                    q_inputs = np.hstack([s_p, tf.keras.utils.to_categorical(a, max_actions_count)])\n",
    "                    q_value = q.predict(np.array([q_inputs]))[0][0]\n",
    "                    if next_chosen_action is None or next_chosen_action_q_value < q_value:\n",
    "                        next_chosen_action = a\n",
    "                        next_chosen_action_q_value = q_value\n",
    "\n",
    "            next_q_inputs = np.hstack([s_p, tf.keras.utils.to_categorical(next_chosen_action, max_actions_count)])\n",
    "            next_chosen_action_q_value = q.predict(np.array([next_q_inputs]))[0][0]\n",
    "\n",
    "            target = r + gamma * next_chosen_action_q_value\n",
    "\n",
    "            q_inputs = np.hstack([s, tf.keras.utils.to_categorical(chosen_action, max_actions_count)])\n",
    "            q.train_on_batch(np.array([q_inputs]), np.array([target]))\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}